#!/usr/bin/env python

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import time
import tf
from geometry_msgs.msg import Point

class ObjectDetector:
    def __init__(self):
        rospy.init_node('object_detector', anonymous=True)

        self.bridge = CvBridge()
        self.cv_color_image = None
        self.cv_depth_image = None

        self.color_image_sub = rospy.Subscriber("/camera/color/image_raw", Image, self.color_image_callback)
        self.depth_image_sub = rospy.Subscriber("/camera/aligned_depth_to_color/image_raw", Image, self.depth_image_callback)

        self.camera_info_sub = rospy.Subscriber("/camera/color/camera_info", CameraInfo, self.camera_info_callback)
        self.tf_listener = tf.TransformListener()

        self.point_pub = rospy.Publisher("goal_point", Point, queue_size=10)
        self.image_pub = rospy.Publisher('detected_cup', Image, queue_size=10)

        self.fx, self.fy, self.cx, self.cy = None, None, None, None
        self.last_position = None
        self.trajectory = []
        self.predicted_trajectory = []

        self.launch_detected = False
        self.threshold = 0.1  # Threshold for launch detection

        rospy.on_shutdown(self.visualize_trajectory)
        rospy.spin()

    def camera_info_callback(self, msg):
        self.fx = msg.K[0]
        self.fy = msg.K[4]
        self.cx = msg.K[2]
        self.cy = msg.K[5]

    def pixel_to_point(self, u, v, depth):
        X = (u - self.cx) * depth / self.fx
        Y = (v - self.cy) * depth / self.fy
        Z = depth
        return X, Y, Z

    def color_image_callback(self, msg):
        try:
            self.cv_color_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            if self.cv_depth_image is not None:
                self.process_images()
        except Exception as e:
            print("Error:", e)

    def depth_image_callback(self, msg):
        try:
            self.cv_depth_image = self.bridge.imgmsg_to_cv2(msg, "16UC1")
        except Exception as e:
            print("Error:", e)

    def process_images(self):
        hsv = cv2.cvtColor(self.cv_color_image, cv2.COLOR_BGR2HSV)
        lower_hsv = np.array([0, 91, 43])
        upper_hsv = np.array([5, 255, 255])
        mask = cv2.inRange(hsv, lower_hsv, upper_hsv)
        y_coords, x_coords = np.nonzero(mask)

        if len(x_coords) == 0 or len(y_coords) == 0:
            return

        center_x = int(np.mean(x_coords))
        center_y = int(np.mean(y_coords))
        depth = self.cv_depth_image[center_y, center_x]

        if self.fx and self.fy and self.cx and self.cy:
            camera_x, camera_y, camera_z = self.pixel_to_point(center_x, center_y, depth)
            camera_link_x, camera_link_y, camera_link_z = camera_z / 1000, -camera_x / 1000, -camera_y / 1000

            # Detect launch based on position change
            current_position = np.array([camera_link_x, camera_link_y, camera_link_z])
            if self.last_position is not None:
                if not self.launch_detected and np.linalg.norm(current_position - self.last_position) > self.threshold:
                    self.launch_detected = True
                    print("Launch detected!")

            if self.launch_detected:
                self.trajectory.append((camera_link_x, camera_link_y, camera_link_z))

                # Predict trajectory based on current data (simple linear model for demonstration)
                if len(self.trajectory) > 1:
                    initial_point = self.trajectory[0]
                    velocity = np.subtract(current_position, initial_point)
                    for t in range(1, 100):  # Predict 100 steps
                        predicted_point = initial_point + velocity * t * (1 / 30)  # Time interval is 1/30 sec
                        self.predicted_trajectory.append(predicted_point)

            self.last_position = current_position

    def visualize_trajectory(self):
        if not self.trajectory or not self.predicted_trajectory:
            print("No trajectory data to visualize.")
            return

        actual_x = [p[0] for p in self.trajectory]
        actual_y = [p[1] for p in self.trajectory]
        actual_z = [p[2] for p in self.trajectory]

        predicted_x = [p[0] for p in self.predicted_trajectory]
        predicted_y = [p[1] for p in self.predicted_trajectory]
        predicted_z = [p[2] for p in self.predicted_trajectory]

        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        ax.plot(actual_x, actual_y, actual_z, label="Actual Trajectory", marker='o')
        ax.plot(predicted_x, predicted_y, predicted_z, label="Predicted Trajectory", linestyle='--')

        ax.set_xlabel("X (m)")
        ax.set_ylabel("Y (m)")
        ax.set_zlabel("Z (m)")
        ax.set_title("3D Ball Trajectory")
        ax.legend()

        # Set plot dimensions to 1m x 1m x 1m
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.set_zlim(0, 1)

        plt.show()

if __name__ == '__main__':
    ObjectDetector()
